{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVHdD6y08F_X",
        "outputId": "c75efe7b-06a9-4de7-aade-fcb226f733b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================\n",
            "シミュレーション回数 N=100000 (Phase=1, Noise=0.0)\n",
            "=======================================\n",
            "先手の学習済み行動分布\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=27417, Estimated Prob=0.819, Q=-1.001\n",
            "  Action Raise/Call: N=6073, Estimated Prob=0.181, Q=-1.032\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=18387, Estimated Prob=0.999, Q=-1.000\n",
            "  Action Raise/Call: N=18, Estimated Prob=0.001, Q=-2.000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Raise/Call: N=608, Estimated Prob=0.018, Q=-0.459\n",
            "  Action Check/Fold: N=32651, Estimated Prob=0.982, Q=-0.283\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=15902, Estimated Prob=0.478, Q=1.239\n",
            "  Action Raise/Call: N=17349, Estimated Prob=0.522, Q=1.176\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=3, Estimated Prob=0.001, Q=-1.000\n",
            "  Action Raise/Call: N=3806, Estimated Prob=0.999, Q=2.000\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=10864, Estimated Prob=0.522, Q=-1.000\n",
            "  Action Raise/Call: N=9932, Estimated Prob=0.478, Q=-1.002\n",
            "\n",
            "後手の学習済み行動分布\n",
            "\n",
            "Card=K (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=29797, Estimated Prob=0.991, Q=1.251\n",
            "  Action Check/Fold (P2): N=269, Estimated Prob=0.009, Q=1.000\n",
            "\n",
            "Card=J (P2), History=(np.int64(1),)\n",
            "  Action Check/Fold (P2): N=8947, Estimated Prob=0.998, Q=-1.000\n",
            "  Action Raise/Call (P2): N=16, Estimated Prob=0.002, Q=-2.000\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=6651, Estimated Prob=0.306, Q=0.160\n",
            "  Action Check/Fold (P2): N=15060, Estimated Prob=0.694, Q=0.179\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=6562, Estimated Prob=0.271, Q=-1.020\n",
            "  Action Check/Fold (P2): N=17631, Estimated Prob=0.729, Q=-1.000\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Check/Fold (P2): N=2, Estimated Prob=0.001, Q=-1.000\n",
            "  Action Raise/Call (P2): N=3341, Estimated Prob=0.999, Q=2.000\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Raise/Call (P2): N=4105, Estimated Prob=0.350, Q=-0.958\n",
            "  Action Check/Fold (P2): N=7619, Estimated Prob=0.650, Q=-1.000\n",
            "=======================================\n",
            "シミュレーション回数 N=1000000 (Phase=1, Noise=0.0)\n",
            "=======================================\n",
            "先手の学習済み行動分布\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=324952, Estimated Prob=0.973, Q=-1.000\n",
            "  Action Raise/Call: N=8998, Estimated Prob=0.027, Q=-1.044\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=168533, Estimated Prob=1.000, Q=-1.000\n",
            "  Action Raise/Call: N=23, Estimated Prob=0.000, Q=-2.000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Raise/Call: N=1690, Estimated Prob=0.005, Q=-0.443\n",
            "  Action Check/Fold: N=331867, Estimated Prob=0.995, Q=-0.328\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=300268, Estimated Prob=0.903, Q=1.177\n",
            "  Action Raise/Call: N=32225, Estimated Prob=0.097, Q=1.153\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=3, Estimated Prob=0.000, Q=-1.000\n",
            "  Action Raise/Call: N=53142, Estimated Prob=1.000, Q=2.000\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=138577, Estimated Prob=0.630, Q=-1.000\n",
            "  Action Raise/Call: N=81272, Estimated Prob=0.370, Q=-1.004\n",
            "\n",
            "後手の学習済み行動分布\n",
            "\n",
            "Card=K (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=327186, Estimated Prob=0.998, Q=1.187\n",
            "  Action Check/Fold (P2): N=680, Estimated Prob=0.002, Q=1.000\n",
            "\n",
            "Card=J (P2), History=(np.int64(1),)\n",
            "  Action Check/Fold (P2): N=16876, Estimated Prob=0.999, Q=-1.000\n",
            "  Action Raise/Call (P2): N=18, Estimated Prob=0.001, Q=-2.000\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=9000, Estimated Prob=0.029, Q=-0.015\n",
            "  Action Check/Fold (P2): N=303307, Estimated Prob=0.971, Q=0.029\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=105364, Estimated Prob=0.332, Q=-1.002\n",
            "  Action Check/Fold (P2): N=211550, Estimated Prob=0.668, Q=-1.000\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Check/Fold (P2): N=2, Estimated Prob=0.000, Q=-1.000\n",
            "  Action Raise/Call (P2): N=5308, Estimated Prob=1.000, Q=2.000\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Raise/Call (P2): N=6539, Estimated Prob=0.316, Q=-0.998\n",
            "  Action Check/Fold (P2): N=14170, Estimated Prob=0.684, Q=-1.000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# =========================\n",
        "# 1. ゲーム設定（元コードと同じ）\n",
        "# =========================\n",
        "cards = [0,1,2]   # J=0, Q=1, K=2\n",
        "card_names = ['J','Q','K']\n",
        "actions = [0,1]   # Check/Fold=0, Bet/Raise/Call=1\n",
        "action_names = ['Check/Fold','Raise/Call']\n",
        "\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.N = 0\n",
        "        self.Q = 0\n",
        "        self.children = {}  # action -> Node\n",
        "\n",
        "trees1 = defaultdict(Node)\n",
        "trees2 = defaultdict(Node)\n",
        "\n",
        "def compute_reward(my_card, opp_card, history):\n",
        "    reward = 0\n",
        "    if len(history)==2:\n",
        "        h0,h1 = history\n",
        "        if h0==0 and h1==0: #Check→Check\n",
        "            reward = 1 if my_card>opp_card else -1\n",
        "        elif h0==1 and h1==0: #Raise→Fold\n",
        "            reward = 1\n",
        "        elif h0==1 and h1==1: #Raise→Call\n",
        "            reward = 2 if my_card>opp_card else -2\n",
        "    elif len(history)==3: #Check→Raise→?\n",
        "        h0,h1,h2 = history\n",
        "        if h0==0 and h1==1:\n",
        "            if h2==0:#先手アクションがFold\n",
        "                reward = -1\n",
        "            else:#先手アクションがCall\n",
        "                reward = 2 if my_card>opp_card else -2\n",
        "    return reward\n",
        "\n",
        "# reward for player_id: keeps same semantics as before\n",
        "def get_reward_for_player(player_id, p1_card, p2_card, history):\n",
        "    p1_view = compute_reward(p1_card, p2_card, history)\n",
        "    return p1_view if player_id == 1 else -p1_view\n",
        "\n",
        "def is_terminal(history):\n",
        "    if history in ([0,0],[1,0],[1,1],[0,1,0],[0,1,1]):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# =========================\n",
        "# 新しいシミュレーション関数（摂動ノイズのオプションを追加）\n",
        "# =========================\n",
        "def simulate_both_mcts(noise_sigma=0.0):\n",
        "    \"\"\"\n",
        "    1プレイアウトで両者とも自己のツリーを用いてUCB選択を行い、\n",
        "    プレイアウト終了後に各プレイヤーの訪問情報でバックアップする。\n",
        "    noise_sigma > 0 の場合、UCBスコアに正規分布ノイズを加える。\n",
        "    \"\"\"\n",
        "    p1_card = np.random.choice(cards)\n",
        "    p2_card = np.random.choice([c for c in cards if c != p1_card])\n",
        "\n",
        "    history = []\n",
        "    visits_p1 = []\n",
        "    visits_p2 = []\n",
        "    Cp = 2.0\n",
        "\n",
        "    while True:\n",
        "        if is_terminal(history) == True:\n",
        "            break\n",
        "\n",
        "        current_player = 1 if len(history) % 2 == 0 else 2\n",
        "\n",
        "        if current_player == 1:\n",
        "            state = (p1_card, tuple(history))\n",
        "            tree = trees1\n",
        "            visits = visits_p1\n",
        "        else:\n",
        "            state = (p2_card, tuple(history))\n",
        "            tree = trees2\n",
        "            visits = visits_p2\n",
        "\n",
        "        node = tree[state]\n",
        "\n",
        "        # 子が未生成／未訪問の行動がある場合はランダムに選ぶ（探索促進）\n",
        "        if node.N == 0 or any(a not in node.children or node.children[a].N == 0 for a in actions):\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            # UCB 選択と摂動ノイズの加算\n",
        "            def calculate_score(a):\n",
        "                child = node.children[a]\n",
        "                # UCB スコアの計算\n",
        "                ucb_score = child.Q + np.sqrt(np.log(node.N + 1) / (child.N + 1) * Cp)\n",
        "\n",
        "                # 摂動フェーズの場合、正規分布からのノイズを加算\n",
        "                if noise_sigma > 0.0:\n",
        "                    # ノイズ N(0, noise_sigma) を抽出\n",
        "                    noise = np.random.normal(0, noise_sigma)\n",
        "                    return ucb_score + noise\n",
        "                else:\n",
        "                    return ucb_score\n",
        "\n",
        "            action = max(actions, key=calculate_score)\n",
        "\n",
        "        # ノードとactionを記録しておき、バックアップ時に使う\n",
        "        if action not in node.children:\n",
        "            node.children[action] = Node()\n",
        "        child_node = node.children[action]\n",
        "\n",
        "        visits.append((node, action, child_node))\n",
        "\n",
        "        history = history + [action]\n",
        "\n",
        "    # ゲーム終了: 報酬を計算（P1視点）\n",
        "    reward_p1 = compute_reward(p1_card, p2_card, history)\n",
        "\n",
        "    # バックアップ\n",
        "    # P1 の訪問更新（報酬は reward_p1）\n",
        "    for node, action, child in visits_p1:\n",
        "        child.N += 1\n",
        "        child.Q += (reward_p1 - child.Q) / child.N\n",
        "        node.N += 1\n",
        "\n",
        "    # P2 の訪問更新（P2視点の報酬は -reward_p1）\n",
        "    for node, action, child in visits_p2:\n",
        "        child.N += 1\n",
        "        child.Q += ((-reward_p1) - child.Q) / child.N\n",
        "        node.N += 1\n",
        "\n",
        "    return reward_p1\n",
        "\n",
        "# =========================\n",
        "# メインループ（3フェーズでの学習）\n",
        "# =========================\n",
        "# 各フェーズのシミュレーション回数\n",
        "N1_OPTIMIZATION = 10000000  # 最適応答に近づける\n",
        "N2_PERTURBATION = 1000000   # 摂動を加える\n",
        "N3_REOPTIMIZATION = 10000000 # 再度最適応答に近づける\n",
        "n_total_simulations = N1_OPTIMIZATION + N2_PERTURBATION + N3_REOPTIMIZATION\n",
        "\n",
        "# 摂動の強さ（標準偏差。UCBスコアに加算される正規分布ノイズの標準偏差）\n",
        "SIGMA_PERTURBATION = 0.5\n",
        "\n",
        "# チェックポイント（結果を表示したいシミュレーション回数）\n",
        "# フェーズの境目を含める\n",
        "checkpoints = [\n",
        "    100000, 1000000,\n",
        "    N1_OPTIMIZATION, # フェーズ1終了時\n",
        "    N1_OPTIMIZATION + 100000, # フェーズ2中盤\n",
        "    N1_OPTIMIZATION + N2_PERTURBATION, # フェーズ2終了時\n",
        "    N1_OPTIMIZATION + N2_PERTURBATION + 1000000, # フェーズ3中盤\n",
        "    n_total_simulations # 全シミュレーション終了時\n",
        "]\n",
        "\n",
        "for i in range(n_total_simulations):\n",
        "\n",
        "    current_sim = i + 1\n",
        "\n",
        "    # フェーズの判定とノイズの設定\n",
        "    if current_sim <= N1_OPTIMIZATION:\n",
        "        # Phase 1: 最適応答 (純粋なUCB)\n",
        "        noise_sigma = 0.0\n",
        "    elif current_sim <= N1_OPTIMIZATION + N2_PERTURBATION:\n",
        "        # Phase 2: 摂動 (UCB + 正規分布ノイズ)\n",
        "        noise_sigma = SIGMA_PERTURBATION\n",
        "    else:\n",
        "        # Phase 3: 再最適応答 (純粋なUCB)\n",
        "        noise_sigma = 0.0\n",
        "\n",
        "    # シミュレーション実行\n",
        "    simulate_both_mcts(noise_sigma=noise_sigma)\n",
        "\n",
        "    if current_sim in checkpoints:\n",
        "        print(f\"=======================================\")\n",
        "        print(f\"シミュレーション回数 N={current_sim} (Phase={1 if current_sim <= N1_OPTIMIZATION else 2 if current_sim <= N1_OPTIMIZATION + N2_PERTURBATION else 3}, Noise={noise_sigma})\")\n",
        "        print(f\"=======================================\")\n",
        "\n",
        "        # 先手の学習済み行動分布\n",
        "        print(\"先手の学習済み行動分布\")\n",
        "        for state, node in trees1.items():\n",
        "            my_card, history = state\n",
        "            total_N = sum(child.N for child in node.children.values())\n",
        "\n",
        "            # N=0のノードは無視\n",
        "            if total_N == 0: continue\n",
        "\n",
        "            print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "            for a, child in node.children.items():\n",
        "                prob = child.N/total_N\n",
        "                print(f\"  Action {action_names[a]}: N={child.N}, Estimated Prob={prob:.3f}, Q={child.Q:.3f}\")\n",
        "\n",
        "        # 後手の学習済み行動分布\n",
        "        print(\"\\n後手の学習済み行動分布\")\n",
        "        for state, node in trees2.items():\n",
        "            my_card, history = state\n",
        "            # P2が判断するのは履歴長が奇数の場合のみ\n",
        "            if len(history) % 2 == 1:\n",
        "                 total_N = sum(child.N for child in node.children.values())\n",
        "\n",
        "                 # N=0のノードは無視\n",
        "                 if total_N == 0: continue\n",
        "\n",
        "                 print(f\"\\nCard={card_names[my_card]} (P2), History={history}\")\n",
        "                 for a, child in node.children.items():\n",
        "                     prob = child.N/total_N\n",
        "                     print(f\"  Action {action_names[a]} (P2): N={child.N}, Estimated Prob={prob:.3f}, Q={child.Q:.3f}\")"
      ]
    }
  ]
}