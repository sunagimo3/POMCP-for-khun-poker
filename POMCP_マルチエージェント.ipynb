{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tCAO29O89peY",
        "outputId": "2e956042-81bb-4899-9435-9c07e893d8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "先手の学習済み行動分布n= 100\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=21, Estimated Prob=0.700\n",
            "  Action Raise/Call: N=9, Estimated Prob=0.300\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=8, Estimated Prob=0.889\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.111\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=21, Estimated Prob=0.583\n",
            "  Action Raise/Call: N=15, Estimated Prob=0.417\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=5, Estimated Prob=0.833\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.167\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=31, Estimated Prob=0.912\n",
            "  Action Raise/Call: N=3, Estimated Prob=0.088\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=2, Estimated Prob=0.182\n",
            "  Action Check/Fold: N=9, Estimated Prob=0.818\n",
            "後手の学習済み行動分布n= 100\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=9, Estimated Prob=0.300\n",
            "  Action Check/Fold (P2): N=21, Estimated Prob=0.700\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=6, Estimated Prob=0.231\n",
            "  Action Check/Fold (P2): N=20, Estimated Prob=0.769\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Raise/Call (P2): N=6, Estimated Prob=0.857\n",
            "  Action Check/Fold (P2): N=1, Estimated Prob=0.143\n",
            "\n",
            "Card=J (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=13, Estimated Prob=0.867\n",
            "  Action Raise/Call (P2): N=2, Estimated Prob=0.133\n",
            "\n",
            "Card=K (P2), History=(0,)\n",
            "  Action Check/Fold (P2): N=6, Estimated Prob=0.353\n",
            "  Action Raise/Call (P2): N=11, Estimated Prob=0.647\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=4, Estimated Prob=0.800\n",
            "  Action Raise/Call (P2): N=1, Estimated Prob=0.200\n",
            "先手の学習済み行動分布n= 1000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=292, Estimated Prob=0.942\n",
            "  Action Raise/Call: N=18, Estimated Prob=0.058\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=63, Estimated Prob=0.453\n",
            "  Action Check/Fold: N=76, Estimated Prob=0.547\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=180, Estimated Prob=0.516\n",
            "  Action Raise/Call: N=169, Estimated Prob=0.484\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=32, Estimated Prob=0.970\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.030\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=316, Estimated Prob=0.927\n",
            "  Action Raise/Call: N=25, Estimated Prob=0.073\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=5, Estimated Prob=0.040\n",
            "  Action Check/Fold: N=121, Estimated Prob=0.960\n",
            "後手の学習済み行動分布n= 1000\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=57, Estimated Prob=0.222\n",
            "  Action Check/Fold (P2): N=200, Estimated Prob=0.778\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=8, Estimated Prob=0.033\n",
            "  Action Check/Fold (P2): N=232, Estimated Prob=0.967\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Raise/Call (P2): N=26, Estimated Prob=0.963\n",
            "  Action Check/Fold (P2): N=1, Estimated Prob=0.037\n",
            "\n",
            "Card=J (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=92, Estimated Prob=0.948\n",
            "  Action Raise/Call (P2): N=5, Estimated Prob=0.052\n",
            "\n",
            "Card=K (P2), History=(0,)\n",
            "  Action Check/Fold (P2): N=58, Estimated Prob=0.199\n",
            "  Action Raise/Call (P2): N=233, Estimated Prob=0.801\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=59, Estimated Prob=0.670\n",
            "  Action Raise/Call (P2): N=29, Estimated Prob=0.330\n",
            "先手の学習済み行動分布n= 10000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=3305, Estimated Prob=0.981\n",
            "  Action Raise/Call: N=65, Estimated Prob=0.019\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=953, Estimated Prob=0.427\n",
            "  Action Check/Fold: N=1281, Estimated Prob=0.573\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=2896, Estimated Prob=0.885\n",
            "  Action Raise/Call: N=378, Estimated Prob=0.115\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=650, Estimated Prob=0.998\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.002\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=3319, Estimated Prob=0.989\n",
            "  Action Raise/Call: N=37, Estimated Prob=0.011\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=12, Estimated Prob=0.008\n",
            "  Action Check/Fold: N=1541, Estimated Prob=0.992\n",
            "後手の学習済み行動分布n= 10000\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=1301, Estimated Prob=0.417\n",
            "  Action Check/Fold (P2): N=1817, Estimated Prob=0.583\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=19, Estimated Prob=0.006\n",
            "  Action Check/Fold (P2): N=3092, Estimated Prob=0.994\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Raise/Call (P2): N=67, Estimated Prob=0.985\n",
            "  Action Check/Fold (P2): N=1, Estimated Prob=0.015\n",
            "\n",
            "Card=J (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=206, Estimated Prob=0.967\n",
            "  Action Raise/Call (P2): N=7, Estimated Prob=0.033\n",
            "\n",
            "Card=K (P2), History=(0,)\n",
            "  Action Check/Fold (P2): N=173, Estimated Prob=0.053\n",
            "  Action Raise/Call (P2): N=3118, Estimated Prob=0.947\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=164, Estimated Prob=0.824\n",
            "  Action Raise/Call (P2): N=35, Estimated Prob=0.176\n",
            "先手の学習済み行動分布n= 100000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=33287, Estimated Prob=0.997\n",
            "  Action Raise/Call: N=94, Estimated Prob=0.003\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=9162, Estimated Prob=0.407\n",
            "  Action Check/Fold: N=13356, Estimated Prob=0.593\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=28637, Estimated Prob=0.859\n",
            "  Action Raise/Call: N=4690, Estimated Prob=0.141\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=5449, Estimated Prob=1.000\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.000\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Check/Fold: N=32281, Estimated Prob=0.970\n",
            "  Action Raise/Call: N=1011, Estimated Prob=0.030\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=18, Estimated Prob=0.001\n",
            "  Action Check/Fold: N=15871, Estimated Prob=0.999\n",
            "後手の学習済み行動分布n= 100000\n",
            "\n",
            "Card=J (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=11504, Estimated Prob=0.372\n",
            "  Action Check/Fold (P2): N=19392, Estimated Prob=0.628\n",
            "\n",
            "Card=Q (P2), History=(np.int64(0),)\n",
            "  Action Raise/Call (P2): N=21, Estimated Prob=0.001\n",
            "  Action Check/Fold (P2): N=30589, Estimated Prob=0.999\n",
            "\n",
            "Card=K (P2), History=(np.int64(1),)\n",
            "  Action Raise/Call (P2): N=553, Estimated Prob=0.998\n",
            "  Action Check/Fold (P2): N=1, Estimated Prob=0.002\n",
            "\n",
            "Card=J (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=2374, Estimated Prob=0.995\n",
            "  Action Raise/Call (P2): N=13, Estimated Prob=0.005\n",
            "\n",
            "Card=K (P2), History=(0,)\n",
            "  Action Check/Fold (P2): N=367, Estimated Prob=0.011\n",
            "  Action Raise/Call (P2): N=32332, Estimated Prob=0.989\n",
            "\n",
            "Card=Q (P2), History=(1,)\n",
            "  Action Check/Fold (P2): N=1940, Estimated Prob=0.680\n",
            "  Action Raise/Call (P2): N=914, Estimated Prob=0.320\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1332828437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0msimulate_both_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1332828437.py\u001b[0m in \u001b[0;36msimulate_both_mcts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# UCB 選択\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             action = max(actions,\n\u001b[0m\u001b[1;32m    100\u001b[0m                      key=lambda a: node.children[a].Q + np.sqrt(np.log(node.N + 1) / (node.children[a].N + 1)*Cp))\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1332828437.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# UCB 選択\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             action = max(actions,\n\u001b[0;32m--> 100\u001b[0;31m                      key=lambda a: node.children[a].Q + np.sqrt(np.log(node.N + 1) / (node.children[a].N + 1)*Cp))\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# ノードとactionを記録しておき、バックアップ時に使う\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# =========================\n",
        "# 1. ゲーム設定（元コードと同じ）\n",
        "# =========================\n",
        "cards = [0,1,2]   # J=0, Q=1, K=2\n",
        "card_names = ['J','Q','K']\n",
        "actions = [0,1]   # Check/Fold=0, Bet/Raise/Call=1\n",
        "action_names = ['Check/Fold','Raise/Call']\n",
        "\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.N = 0\n",
        "        self.Q = 0\n",
        "        self.children = {}  # action -> Node\n",
        "\n",
        "trees1 = defaultdict(Node)\n",
        "trees2 = defaultdict(Node)\n",
        "\n",
        "def compute_reward(my_card, opp_card, history):\n",
        "    reward = 0\n",
        "    if len(history)==2:\n",
        "        h0,h1 = history\n",
        "        if h0==0 and h1==0: #Check→Check\n",
        "            reward = 1 if my_card>opp_card else -1\n",
        "        elif h0==1 and h1==0: #Raise→Fold\n",
        "            reward = 1\n",
        "        elif h0==1 and h1==1: #Raise→Call\n",
        "            reward = 2 if my_card>opp_card else -2\n",
        "    elif len(history)==3: #Check→Raise→?\n",
        "        h0,h1,h2 = history\n",
        "        if h0==0 and h1==1:\n",
        "            if h2==0:#先手アクションがFold\n",
        "                reward = -1\n",
        "            else:#先手アクションがCall\n",
        "                reward = 2 if my_card>opp_card else -2\n",
        "    return reward\n",
        "\n",
        "# reward for player_id: keeps same semantics as before\n",
        "def get_reward_for_player(player_id, p1_card, p2_card, history):\n",
        "    p1_view = compute_reward(p1_card, p2_card, history)\n",
        "    return p1_view if player_id == 1 else -p1_view\n",
        "\n",
        "def is_terminal(history):\n",
        "  if history in ([0,0],[1,0],[1,1],[0,1,0],[0,1,1]):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "# =========================\n",
        "# 新しいシミュレーション関数（両プレイヤーがUCBで行動選択し学習する）\n",
        "# =========================\n",
        "def simulate_both_mcts():\n",
        "    \"\"\"\n",
        "    1プレイアウトで両者とも自己のツリーを用いてUCB選択を行い、\n",
        "    プレイアウト終了後に各プレイヤーの訪問情報でバックアップする。\n",
        "    \"\"\"\n",
        "    # サンプリングされた private cards（1回のプレイアウトにつき固定）\n",
        "    p1_card = np.random.choice(cards)\n",
        "    p2_card = np.random.choice([c for c in cards if c != p1_card])\n",
        "\n",
        "    history = []\n",
        "\n",
        "    # 訪問記録: player -> list of (node, action, child_node)\n",
        "    visits_p1 = []\n",
        "    visits_p2 = []\n",
        "\n",
        "    # どちらのプレイヤが選択するかは履歴長に依存する\n",
        "    # 履歴が偶数長ならP1の番（履歴長=0,2 -> P1 が決定。注意: 履歴長=2は通常ゲーム終了だが条件で扱う）\n",
        "    # 履歴が奇数長ならP2の番\n",
        "    # ただし、このゲームの終端は履歴長が2または3になるタイミングなのでループでそれをチェック\n",
        "    Cp = 2.0\n",
        "\n",
        "    while True:\n",
        "        if is_terminal(history) == True:\n",
        "          break\n",
        "\n",
        "        # 決定者を決める\n",
        "        current_player = 1 if len(history) % 2 == 0 else 2\n",
        "\n",
        "        # 状態キーは (player_card, tuple(history))\n",
        "        if current_player == 1:\n",
        "            state = (p1_card, tuple(history))\n",
        "            tree = trees1\n",
        "            visits = visits_p1\n",
        "        else:\n",
        "            state = (p2_card, tuple(history))\n",
        "            tree = trees2\n",
        "            visits = visits_p2\n",
        "\n",
        "        node = tree[state]  # defaultdict によってノードが自動生成される\n",
        "\n",
        "        # 子が未生成／未訪問の行動がある場合はランダムに選ぶ（探索促進）\n",
        "        if node.N == 0 or any(a not in node.children or node.children[a].N == 0 for a in actions):\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            # UCB 選択\n",
        "            action = max(actions,\n",
        "                     key=lambda a: node.children[a].Q + np.sqrt(np.log(node.N + 1) / (node.children[a].N + 1)*Cp))\n",
        "\n",
        "        # ノードとactionを記録しておき、バックアップ時に使う\n",
        "        # 子ノードがなければ作る（即座にN/Q更新可能）\n",
        "        if action not in node.children:\n",
        "            node.children[action] = Node()\n",
        "        child_node = node.children[action]\n",
        "\n",
        "        visits.append((node, action, child_node))\n",
        "\n",
        "        # 実際に行動を反映\n",
        "        history = history + [action]\n",
        "\n",
        "        # 特殊ケース: もし Check -> Raise -> (P1 の返し) などで次がゲーム終了になればループが抜ける\n",
        "\n",
        "        # ループ続行して次のプレイヤの判断へ（両者とも同じロジックで選択を行う）\n",
        "        # ここでは\"ロールアウト\"部を単純化しており、ランダムポリシーや固定ロールアウトは入れていない。\n",
        "        # （よりPOMCPに忠実にするには、未展開ノードに出会ったら確率的ロールアウトを走らせる実装を追加できます。）\n",
        "\n",
        "    # ゲーム終了: 報酬を計算（P1視点）\n",
        "    reward_p1 = compute_reward(p1_card, p2_card, history)\n",
        "\n",
        "    # 各訪問リストについてバックアップ（各プレイヤー視点の報酬で）\n",
        "    # P1 の訪問更新（報酬は reward_p1）\n",
        "    for node, action, child in visits_p1:\n",
        "        child.N += 1\n",
        "        # 子の Q を incremental update（平均）で更新\n",
        "        child.Q += (reward_p1 - child.Q) / child.N\n",
        "        node.N += 1\n",
        "\n",
        "    # P2 の訪問更新（P2視点の報酬は -reward_p1）\n",
        "    for node, action, child in visits_p2:\n",
        "        child.N += 1\n",
        "        child.Q += ((-reward_p1) - child.Q) / child.N\n",
        "        node.N += 1\n",
        "\n",
        "    # 戻り値は P1 視点の報酬（必要なら呼び出し側で使う）\n",
        "    return reward_p1\n",
        "\n",
        "# =========================\n",
        "# メインループ（両者同時に学習）\n",
        "# =========================\n",
        "n_simulations = 100000000  # 必要に応じて増やしてください（100kでも可だが実行時間に注意）\n",
        "checkpoints = [100, 1000, 10000, 100000, 1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000,100000000]\n",
        "rewards_all = []\n",
        "\n",
        "for i in range(n_simulations):\n",
        "    simulate_both_mcts()\n",
        "    if (i+1) in checkpoints:\n",
        "      # =========================\n",
        "      # 結果表示（元の出力部をそのまま使用できます）\n",
        "      # 以下は元のコードと同様の出力ロジックを使って trees1 / trees2 を表示できます\n",
        "      # （出力部分は長いので省略せずに使ってください — ここでは例として先手の分布を表示）\n",
        "      # =========================\n",
        "\n",
        "      print(\"先手の学習済み行動分布n=\",i+1)\n",
        "      for state, node in trees1.items():\n",
        "          my_card, history = state\n",
        "          total_N = sum(child.N for child in node.children.values())\n",
        "          print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "          for a, child in node.children.items():\n",
        "              prob = child.N/total_N if total_N>0 else 0\n",
        "              print(f\"  Action {action_names[a]}: N={child.N}, Estimated Prob={prob:.3f}\")\n",
        "\n",
        "      print(\"後手の学習済み行動分布n=\",i+1)\n",
        "      for state, node in trees2.items():\n",
        "          my_card, history = state\n",
        "          if len(history) == 1:\n",
        "              total_N = sum(child.N for child in node.children.values())\n",
        "              print(f\"\\nCard={card_names[my_card]} (P2), History={history}\")\n",
        "              for a, child in node.children.items():\n",
        "                  prob = child.N/total_N if total_N>0 else 0\n",
        "                  print(f\"  Action {action_names[a]} (P2): N={child.N}, Estimated Prob={prob:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coqJ5U1QVLtd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}