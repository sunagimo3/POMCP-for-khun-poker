モンテカルロ木探索(MCTS)はそもそも
１．Selection(UCBなどの次ノード遷移アルゴリズムを用いることで未展開ノードに到達するまで木をたどる)
２．Expansion(未展開の子ノードを1つつくる)
３．Simulation(Rollout)(ランダム方策で終点までサンプル)
４．Baskpropagation(得点を逆伝播して更新)

今のコードは「モンテカルロ木探索(MCTS)」の形式にはなっている．ExpansionとSimulationのフェーズがほぼ動いておらず，実質的には「多腕バンディットのUCBを描く情報集合ごとに学習しているアルゴリズム」となっている．

なぜか？
１．Selection→UCBによる選択を行っている
２．Expansion→数ステップで全情報集合が生成されつくす
３．Simulation→シュミレーションによる推定でなく，ゲームの実際の遷移だけで平均報酬を学んでいる
４．Backpropagation→訪問ノードに報酬を逆伝播して平均化している→MCTSのバックアップは成立している

本来のMCTSは，
・木の深さを徐々に深める
・未知のノードを「rollout」で推定する
という逐次拡大型の探索アルゴリズム

結論として「MCTSがゲーム木を探索する必要がないほど単純なゲームに適用された結果，内部構造がUCBベースの反復強化学習に縮退した」





このコードにおいて，信念の更新は一切存在していない
・POMCPでは本来，「観測から推測される相手の手札の確率分布」を粒子フィルタで更新する．
・状態を(自分のカード，history)とし，相手のカードは固定的に1枚サンプリングした後使いまわしているだけで，簡素ｋから状態後分布を絞る処理がない
・本質的には「完全情報ゲームに対するMCTS(UCB付き)」をしているだけで，ＰＯＭＤＰの「信念更新」には該当しない




POMCPは，各ノード(履歴)に対して，
B(h) = {s^(1),s^(2),...}
という「粒子集合(=ありうる状態サンプル)」を保持し，
・次の観測と矛盾する粒子を排除し
・観測と整合する粒子のみを次ノードに追加する
ことで信念のベイズ更新を粒子で近似している



マルチアームバンディット問題
強化学習において状態Sが変化しない最も単純な問題である．行動の主体であるエージェントは腕を引くという行動だけ行う．どの腕を選んだらよいのか，行動を通じて探索していく．