{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsrRPHqRKHh4",
        "outputId": "95c09c44-ea76-4112-b797-61ac4b16a41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "先手の学習済み行動分布 (各カード・状態ごと):\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check: N=1142, Estimated Prob=0.698\n",
            "  Action Bet: N=494, Estimated Prob=0.302\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Bet: N=12, Estimated Prob=0.007\n",
            "  Action Check: N=1681, Estimated Prob=0.993\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check: N=1, Estimated Prob=0.001\n",
            "  Action Bet: N=1670, Estimated Prob=0.999\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRywEyJHDFtc",
        "outputId": "27b21523-a07c-428b-8113-6dd4909618fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "先手の学習済み行動分布 (カード・履歴ごと):\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Raise: N=17, Estimated Prob=0.001\n",
            "  Action Check: N=16648, Estimated Prob=0.999\n",
            "\n",
            "Card=J, History=(np.int64(1), np.int64(1))\n",
            "  Action Check: N=9, Estimated Prob=0.529\n",
            "  Action Raise: N=8, Estimated Prob=0.471\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Raise: N=16659, Estimated Prob=1.000\n",
            "  Action Check: N=2, Estimated Prob=0.000\n",
            "\n",
            "Card=K, History=(np.int64(1), np.int64(1))\n",
            "  Action Check: N=6243, Estimated Prob=0.500\n",
            "  Action Raise: N=6243, Estimated Prob=0.500\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Raise: N=16382, Estimated Prob=0.982\n",
            "  Action Check: N=292, Estimated Prob=0.018\n",
            "\n",
            "Card=Q, History=(np.int64(1), np.int64(1))\n",
            "  Action Check: N=6147, Estimated Prob=0.500\n",
            "  Action Raise: N=6147, Estimated Prob=0.500\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Check: N=76, Estimated Prob=0.349\n",
            "  Action Raise: N=142, Estimated Prob=0.651\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Check: N=16631, Estimated Prob=0.999\n",
            "  Action Raise: N=17, Estimated Prob=0.001\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Check: N=2, Estimated Prob=1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#辞書キーが存在しない場合でも，自動でデフォルト値を生成できる辞書を作るために使用\n",
        "from collections import defaultdict\n",
        "\n",
        "# =========================\n",
        "# 1. ゲーム設定\n",
        "# =========================\n",
        "cards = [0,1,2]   # J=0, Q=1, K=2\n",
        "card_names = ['J','Q','K']\n",
        "actions = [0,1]   # Check/Fold=0, Bet/Raise/Call=1\n",
        "action_names = ['Check/Fold','Raise/Call']\n",
        "\n",
        "# =========================\n",
        "# 2. 後手固定Nash方策\n",
        "# =========================\n",
        "# 状態: ('先手のアクション') -> カード -> 後手行動確率\n",
        "# 先手Check直後\n",
        "nash_second_player_after_check = {\n",
        "    0: [2/3, 1/3],  # J: Check 2/3, Raise 1/3\n",
        "    1: [1.0, 0.0],  # Q: Check 1, Raise 0\n",
        "    2: [0.0, 1.0],  # K: Check 0, Raise 1\n",
        "}\n",
        "\n",
        "# 先手Raise直後\n",
        "nash_second_player_after_raise = {\n",
        "    0: [1.0, 0.0],    # J: Fold 1, Call 0\n",
        "    1: [2/3, 1/3],    # Q: Fold 2/3, Call 1/3\n",
        "    2: [0.0, 1.0],    # K: Fold 0, Call 1\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 3. POMCP用ノード\n",
        "# =========================\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.N = 0\n",
        "        self.Q = 0\n",
        "        self.children = {}  # action -> Node\n",
        "\n",
        "trees = defaultdict(Node)\n",
        "\n",
        "# =========================\n",
        "# 4. 報酬計算\n",
        "# =========================\n",
        "def compute_reward(my_card, opp_card, history):\n",
        "    reward = 0\n",
        "    if len(history)==2:\n",
        "        h0,h1 = history\n",
        "        if h0==0 and h1==0: #Check→Check\n",
        "            reward = 1 if my_card>opp_card else -1\n",
        "        elif h0==1 and h1==0: #Raise→Fold\n",
        "            reward = 1\n",
        "        elif h0==1 and h1==1: #Raise→Call\n",
        "            reward = 2 if my_card>opp_card else -2\n",
        "    elif len(history)==3: #Check→Raise→?\n",
        "        h0,h1,h2 = history\n",
        "        if h0==0 and h1==1:\n",
        "            # 先手Check → 後手Raise → 先手アクション\n",
        "            if h2==0:#先手アクションがFold\n",
        "                reward = -1\n",
        "            else:#先手アクションがCall\n",
        "                reward = 2 if my_card>opp_card else -2\n",
        "    return reward\n",
        "\n",
        "# =========================\n",
        "# 5. シミュレーション\n",
        "# =========================\n",
        "\n",
        "#POMCPといいつつ，探索が深くないので全探索できちゃったやつ\n",
        "#POMCPのシュミレーション1回を行う関数\n",
        "def simulate(my_card, history):\n",
        "    # opp_card∊{0,1,2}(my_card以外)，opp_action∊{0,1}\n",
        "    opp_card = np.random.choice([c for c in cards if c != my_card])\n",
        "    #state=my_cardと履歴(history)の組み合わせ\n",
        "    state = (my_card, tuple(history))\n",
        "    #その状態に対応するノードを木構造treesから取得する\n",
        "    node = trees[state]\n",
        "\n",
        "    # 先手のアクション選択（既存ロジックそのまま）\n",
        "    if node.N == 0 or any(a not in node.children or node.children[a].N == 0 for a in actions):\n",
        "        action = np.random.choice(actions)\n",
        "    else:\n",
        "        action = max(actions, key=lambda a: node.children[a].Q + np.sqrt(np.log(node.N + 1) / (node.children[a].N + 1)))\n",
        "\n",
        "    history_next = history + [action]\n",
        "\n",
        "    # 後手の最初のアクションを、先手の直前アクションに応じて選ぶ（ここが修正点）\n",
        "    if len(history_next) == 1:\n",
        "        if history_next[0] == 0:  # 先手がCheckした後は，「先手がcheckした後」の確率で\n",
        "            p = nash_second_player_after_check[opp_card]\n",
        "        else:  # 先手がRaiseした後は，「先手がRaiseした後」の確率で\n",
        "            p = nash_second_player_after_raise[opp_card]\n",
        "        opp_action = np.random.choice(actions, p=p)\n",
        "        history_next.append(opp_action)\n",
        "\n",
        "    # ここで最終報酬を決めるが、Check->Raise の場合は「さらに先手の返し」があるため扱いを分ける\n",
        "    reward_total = None\n",
        "\n",
        "    # Check -> Raise -> (先手の返し) の場合\n",
        "    if len(history_next) == 2 and history_next[0] == 0 and history_next[1] == 1:\n",
        "        # 次の状態ノード（先手が返す局面）を取得／選択\n",
        "        next_state = (my_card, tuple(history_next))\n",
        "        next_node = trees[next_state]\n",
        "        # この部分は先のバンディットによる探索と同じアルゴリズムで動く\n",
        "        # 未探索な部分に対するランダムな探索アルゴリズム\n",
        "        if next_node.N == 0 or any(a not in next_node.children or next_node.children[a].N == 0 for a in actions):\n",
        "            next_action = np.random.choice(actions)\n",
        "        # 探索済みの部分に対するUCBによる探索アルゴリズム(知識利用部分)\n",
        "        else:\n",
        "            next_action = max(actions, key=lambda a: next_node.children[a].Q + np.sqrt(np.log(next_node.N + 1) / (next_node.children[a].N + 1)))\n",
        "        #history_finalはこれまでのアクションの履歴と次のアクションの合算になっており，history_next==2で条件分岐したのでhistory_finalの大きさは3である．\n",
        "        history_final = history_next + [next_action]\n",
        "        reward_total = compute_reward(my_card, opp_card, history_final)\n",
        "\n",
        "        # next_node（先手の返しのノード）を更新\n",
        "        # ここでは自分のカードとこれまでのアクション履歴を合わせた状況におけるノードに対する学習を行っている\n",
        "        if next_action not in next_node.children:\n",
        "            next_node.children[next_action] = Node()\n",
        "        child2 = next_node.children[next_action]\n",
        "        child2.N += 1\n",
        "        child2.Q += (reward_total - child2.Q) / child2.N\n",
        "        next_node.N += 1\n",
        "\n",
        "    else:\n",
        "        # 通常ケース（Raise->Fold/Call や Check->Check など）では、今の history_next で終了\n",
        "        reward_total = compute_reward(my_card, opp_card, history_next)\n",
        "\n",
        "    # ここで「最初のノード配下の子（action）」を最終報酬で更新\n",
        "    # node.children[action]はその行動をとった後に遷移する子ノードで，子ノードはその行動をとった回数や平均報酬を保持する\n",
        "    if action not in node.children:\n",
        "        node.children[action] = Node()\n",
        "    child = node.children[action]\n",
        "    # その行動を選んだ回数を1増やす\n",
        "    # その行動の平均報酬を逐次更新する\n",
        "    # 親ノードの訪問回数も1増やす\n",
        "    child.N += 1\n",
        "    child.Q += (reward_total - child.Q) / child.N\n",
        "    node.N += 1\n",
        "\n",
        "    return reward_total\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. POMCP 実行\n",
        "# =========================\n",
        "n_simulations = 500000\n",
        "for _ in range(n_simulations):\n",
        "    my_card = np.random.choice(cards)\n",
        "    simulate(my_card, [])\n",
        "\n",
        "# =========================\n",
        "# 7. 結果表示\n",
        "# =========================\n",
        "print(\"先手の学習済み行動分布 (カード・履歴ごと):\")\n",
        "for state, node in trees.items():\n",
        "    my_card, history = state\n",
        "    total_N = sum(child.N for child in node.children.values())\n",
        "    print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "    for a, child in node.children.items():\n",
        "        prob = child.N/total_N if total_N>0 else 0\n",
        "        print(f\"  Action {action_names[a]}: N={child.N}, Estimated Prob={prob:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# 8. 期待値表示\n",
        "# =========================\n",
        "print(\"\\n先手の期待値 (カード・履歴ごと):\")\n",
        "for state, node in trees.items():\n",
        "    my_card, history = state\n",
        "    total_N = sum(child.N for child in node.children.values())\n",
        "    print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "    for a, child in node.children.items():\n",
        "        # child.Q がその action からの平均報酬の推定値\n",
        "        print(f\"  Action {action_names[a]}: Estimated Value={child.Q:.3f}, N={child.N}, Prob={child.N/total_N:.3f}\")\n",
        "\n",
        "    # 状態全体の期待値 = すべての行動の期待値の平均\n",
        "    if total_N > 0:\n",
        "        expected_value = sum((child.N/total_N)*child.Q for child in node.children.values())\n",
        "        print(f\"  -> Expected Value of this state: {expected_value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txf4SyzBI9yr",
        "outputId": "2c15e42d-4a50-42fc-f3f2-dccf0a9e28a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "先手の学習済み行動分布 (カード・履歴ごと):\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=166051, Estimated Prob=0.996\n",
            "  Action Raise/Call: N=749, Estimated Prob=0.004\n",
            "\n",
            "Card=Q, History=(0, np.int64(1))\n",
            "  Action Check/Fold: N=108988, Estimated Prob=0.985\n",
            "  Action Raise/Call: N=1679, Estimated Prob=0.015\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: N=98858, Estimated Prob=0.593\n",
            "  Action Raise/Call: N=67764, Estimated Prob=0.407\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: N=16358, Estimated Prob=1.000\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.000\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Raise/Call: N=48504, Estimated Prob=0.291\n",
            "  Action Check/Fold: N=118074, Estimated Prob=0.709\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=58940, Estimated Prob=1.000\n",
            "  Action Raise/Call: N=10, Estimated Prob=0.000\n",
            "\n",
            "先手の期待値 (カード・履歴ごと):\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: Estimated Value=-0.334, N=166051, Prob=0.996\n",
            "  Action Raise/Call: Estimated Value=-0.454, N=749, Prob=0.004\n",
            "  -> Expected Value of this state: -0.334\n",
            "\n",
            "Card=Q, History=(0, np.int64(1))\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=108988, Prob=0.985\n",
            "  Action Raise/Call: Estimated Value=-1.073, N=1679, Prob=0.015\n",
            "  -> Expected Value of this state: -1.001\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Check/Fold: Estimated Value=1.165, N=98858, Prob=0.593\n",
            "  Action Raise/Call: Estimated Value=1.164, N=67764, Prob=0.407\n",
            "  -> Expected Value of this state: 1.165\n",
            "\n",
            "Card=K, History=(np.int64(0), np.int64(1))\n",
            "  Action Raise/Call: Estimated Value=2.000, N=16358, Prob=1.000\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=1, Prob=0.000\n",
            "  -> Expected Value of this state: 2.000\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Raise/Call: Estimated Value=-1.006, N=48504, Prob=0.291\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=118074, Prob=0.709\n",
            "  -> Expected Value of this state: -1.002\n",
            "\n",
            "Card=J, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=58940, Prob=1.000\n",
            "  Action Raise/Call: Estimated Value=-2.000, N=10, Prob=0.000\n",
            "  -> Expected Value of this state: -1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# =========================\n",
        "# 1. ゲーム設定\n",
        "# =========================\n",
        "cards = [0,1,2]   # J=0, Q=1, K=2\n",
        "card_names = ['J','Q','K']\n",
        "actions = [0,1]   # Check/Fold=0, Raise/Call=1\n",
        "action_names = ['Check/Fold','Raise/Call']\n",
        "\n",
        "# =========================\n",
        "# 2. 後手固定Nash方策\n",
        "# =========================\n",
        "# 先手Check直後\n",
        "nash_second_player_after_check = {\n",
        "    0: [2/3, 1/3],  # J: Check 2/3, Raise 1/3\n",
        "    1: [1.0, 0.0],  # Q: Check 1, Raise 0\n",
        "    2: [0.0, 1.0],  # K: Check 0, Raise 1\n",
        "}\n",
        "\n",
        "# 先手Raise直後\n",
        "nash_second_player_after_raise = {\n",
        "    0: [1.0, 0.0],    # J: Fold 1, Call 0\n",
        "    1: [2/3, 1/3],    # Q: Fold 2/3, Call 1/3\n",
        "    2: [0.0, 1.0],    # K: Fold 0, Call 1\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 3. POMCP用ノード定義\n",
        "# =========================\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.N = 0\n",
        "        self.Q = 0\n",
        "        self.children = {}  # action -> Node\n",
        "\n",
        "trees = defaultdict(Node)\n",
        "\n",
        "# =========================\n",
        "# 4. 報酬計算関数\n",
        "# =========================\n",
        "def compute_reward(my_card, opp_card, history):\n",
        "    reward = 0\n",
        "    if len(history)==2:\n",
        "        h0,h1 = history\n",
        "        if h0==0 and h1==0:\n",
        "            reward = 1 if my_card>opp_card else -1\n",
        "        elif h0==0 and h1==1:\n",
        "            reward = -1\n",
        "        elif h0==1 and h1==0:\n",
        "            reward = 1\n",
        "        elif h0==1 and h1==1:\n",
        "            reward = 2 if my_card>opp_card else -2\n",
        "    elif len(history)==3:\n",
        "        h0,h1,h2 = history\n",
        "        if h0==0 and h1==1:\n",
        "            reward = -1 if h2==0 else (2 if my_card>opp_card else -2)\n",
        "    return reward\n",
        "\n",
        "# =========================\n",
        "# 5. POMCPシミュレーション\n",
        "# =========================\n",
        "def simulate(my_card, history):\n",
        "    # 後手のカード\n",
        "    opp_card = np.random.choice([c for c in cards if c!=my_card])\n",
        "\n",
        "    # 現在ノード\n",
        "    state = (my_card, tuple(history))\n",
        "    node = trees[state]\n",
        "\n",
        "    # 先手アクション選択（UCB or ランダム）\n",
        "    if node.N==0 or any(a not in node.children or node.children[a].N==0 for a in actions):\n",
        "        action = np.random.choice(actions)\n",
        "    else:\n",
        "        action = max(actions, key=lambda a: node.children[a].Q + np.sqrt(np.log(node.N+1)/(node.children[a].N+1)))\n",
        "\n",
        "    # 履歴更新\n",
        "    history_next = history + [action]\n",
        "\n",
        "    # 後手のアクション\n",
        "    if len(history_next)==1:\n",
        "        opp_action = np.random.choice(actions, p=nash_second_player_after_check[opp_card])\n",
        "        history_next.append(opp_action)\n",
        "    elif len(history_next)==2 and history_next[0]==1:\n",
        "        opp_action = np.random.choice(actions, p=nash_second_player_after_raise[opp_card])\n",
        "        history_next.append(opp_action)\n",
        "\n",
        "    # 報酬計算\n",
        "    reward = compute_reward(my_card, opp_card, history_next)\n",
        "\n",
        "    # ノード更新\n",
        "    if action not in node.children:\n",
        "        node.children[action] = Node()\n",
        "    child = node.children[action]\n",
        "    child.N += 1\n",
        "    child.Q += (reward - child.Q)/child.N\n",
        "    node.N += 1\n",
        "\n",
        "    # Check → 後手Raise → 先手アクションの更新\n",
        "    if len(history_next)==2 and history_next[0]==0 and history_next[1]==1:\n",
        "        next_state = (my_card, tuple(history_next))\n",
        "        next_node = trees[next_state]\n",
        "        if next_node.N==0 or any(a not in next_node.children or next_node.children[a].N==0 for a in actions):\n",
        "            next_action = np.random.choice(actions)\n",
        "        else:\n",
        "            next_action = max(actions, key=lambda a: next_node.children[a].Q + np.sqrt(np.log(next_node.N+1)/(next_node.children[a].N+1)))\n",
        "        history_final = history_next + [next_action]\n",
        "        reward2 = compute_reward(my_card, opp_card, history_final)\n",
        "        if next_action not in next_node.children:\n",
        "            next_node.children[next_action] = Node()\n",
        "        child2 = next_node.children[next_action]\n",
        "        child2.N += 1\n",
        "        child2.Q += (reward2 - child2.Q)/child2.N\n",
        "        next_node.N += 1\n",
        "\n",
        "    return reward\n",
        "\n",
        "# =========================\n",
        "# 6. シミュレーション実行\n",
        "# =========================\n",
        "n_simulations = 500000\n",
        "for _ in range(n_simulations):\n",
        "    my_card = np.random.choice(cards)\n",
        "    simulate(my_card, [])\n",
        "\n",
        "# =========================\n",
        "# 7. 学習済み行動分布表示\n",
        "# =========================\n",
        "print(\"先手の学習済み行動分布 (カード・履歴ごと):\")\n",
        "for state, node in trees.items():\n",
        "    my_card, history = state\n",
        "    total_N = sum(child.N for child in node.children.values())\n",
        "    print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "    for a, child in node.children.items():\n",
        "        prob = child.N/total_N if total_N>0 else 0\n",
        "        print(f\"  Action {action_names[a]}: N={child.N}, Estimated Prob={prob:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# 8. 先手期待値表示\n",
        "# =========================\n",
        "print(\"\\n先手の期待値 (カード・履歴ごと):\")\n",
        "for state, node in trees.items():\n",
        "    my_card, history = state\n",
        "    total_N = sum(child.N for child in node.children.values())\n",
        "    print(f\"\\nCard={card_names[my_card]}, History={history}\")\n",
        "    for a, child in node.children.items():\n",
        "        print(f\"  Action {action_names[a]}: Estimated Value={child.Q:.3f}, N={child.N}, Prob={child.N/total_N:.3f}\")\n",
        "\n",
        "    if total_N>0:\n",
        "        expected_value = sum((child.N/total_N)*child.Q for child in node.children.values())\n",
        "        print(f\"  -> Expected Value of this state: {expected_value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTePiab3P2oj",
        "outputId": "805d6854-77b3-4af1-f68d-5e24c9b2c86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "先手の学習済み行動分布 (カード・履歴ごと):\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Raise/Call: N=166888, Estimated Prob=0.999\n",
            "  Action Check/Fold: N=88, Estimated Prob=0.001\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Raise/Call: N=166545, Estimated Prob=1.000\n",
            "  Action Check/Fold: N=43, Estimated Prob=0.000\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: N=148399, Estimated Prob=0.892\n",
            "  Action Raise/Call: N=18037, Estimated Prob=0.108\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: N=89035, Estimated Prob=0.900\n",
            "  Action Raise/Call: N=9849, Estimated Prob=0.100\n",
            "\n",
            "Card=J, History=(0, np.int64(1))\n",
            "  Action Check/Fold: N=41, Estimated Prob=0.953\n",
            "  Action Raise/Call: N=2, Estimated Prob=0.047\n",
            "\n",
            "Card=K, History=(0, np.int64(1))\n",
            "  Action Check/Fold: N=1, Estimated Prob=0.125\n",
            "  Action Raise/Call: N=7, Estimated Prob=0.875\n",
            "\n",
            "先手の期待値 (カード・履歴ごと):\n",
            "\n",
            "Card=J, History=()\n",
            "  Action Raise/Call: Estimated Value=-0.500, N=166888, Prob=0.999\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=88, Prob=0.001\n",
            "  -> Expected Value of this state: -0.500\n",
            "\n",
            "Card=K, History=()\n",
            "  Action Raise/Call: Estimated Value=1.167, N=166545, Prob=1.000\n",
            "  Action Check/Fold: Estimated Value=0.628, N=43, Prob=0.000\n",
            "  -> Expected Value of this state: 1.167\n",
            "\n",
            "Card=Q, History=()\n",
            "  Action Check/Fold: Estimated Value=-0.333, N=148399, Prob=0.892\n",
            "  Action Raise/Call: Estimated Value=-0.350, N=18037, Prob=0.108\n",
            "  -> Expected Value of this state: -0.335\n",
            "\n",
            "Card=Q, History=(np.int64(0), np.int64(1))\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=89035, Prob=0.900\n",
            "  Action Raise/Call: Estimated Value=-1.023, N=9849, Prob=0.100\n",
            "  -> Expected Value of this state: -1.002\n",
            "\n",
            "Card=J, History=(0, np.int64(1))\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=41, Prob=0.953\n",
            "  Action Raise/Call: Estimated Value=-2.000, N=2, Prob=0.047\n",
            "  -> Expected Value of this state: -1.047\n",
            "\n",
            "Card=K, History=(0, np.int64(1))\n",
            "  Action Check/Fold: Estimated Value=-1.000, N=1, Prob=0.125\n",
            "  Action Raise/Call: Estimated Value=2.000, N=7, Prob=0.875\n",
            "  -> Expected Value of this state: 1.625\n"
          ]
        }
      ]
    }
  ]
}
